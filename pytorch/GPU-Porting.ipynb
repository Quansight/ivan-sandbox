{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Porting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/xmn/dev/quansight/tmp/pytorch/output'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "pytorch_path = '/home/xmn/dev/quansight/pytorch-project/pytorch'\n",
    "thcunn_path = os.path.join(pytorch_path, 'aten/src/THCUNN')\n",
    "at_cuda_path = os.path.join(pytorch_path, 'aten/src/ATen/native/cuda')\n",
    "\n",
    "thcunn_files = [\n",
    "    'SpatialUpSamplingBicubic.cu',\n",
    "    'SpatialUpSamplingBilinear.cu',\n",
    "    'SpatialUpSamplingNearest.cu',\n",
    "    'TemporalUpSamplingLinear.cu',\n",
    "    'TemporalUpSamplingNearest.cu',\n",
    "    'VolumetricUpSamplingNearest.cu',\n",
    "    'VolumetricUpSamplingTrilinear.cu'\n",
    "]\n",
    "thcunn_h_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove `aten/src/THNN/generic/*.c` files that is being ported\n",
    "# Remove functions to be ported from:\n",
    "# `/aten/src/THCUNN/CMakeLists.txt`\n",
    "# `/aten/src/THCUNN/generic/THCUNN.h`\n",
    "# `/aten/src/THNN/init.cpp`\n",
    "# `/aten/src/ATen/nn.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UpSampleBicubic2d.cu',\n",
       " 'UpSampleBilinear2d.cu',\n",
       " 'UpSampleNearest2d.cu',\n",
       " 'UpSampleLinear1d.cu',\n",
       " 'UpSampleNearest1d.cu',\n",
       " 'UpSampleNearest3d.cu',\n",
       " 'UpSampleTrilinear3d.cu']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _remove_ext(v):\n",
    "    if '.' in v:\n",
    "        return v.split('.')[0]\n",
    "    return v\n",
    "\n",
    "def _get_ext(v):\n",
    "    if '.' in v:\n",
    "        return '.' + v.split('.')[-1]\n",
    "    return ''\n",
    "    \n",
    "RULES_NAME = [\n",
    "    lambda v, w='Temporal': (\n",
    "        _remove_ext(v).replace(w, '') + '1d' + _get_ext(v)\n",
    "        if v.startswith(w)\n",
    "        else v\n",
    "    ),\n",
    "    lambda v, w='Spatial': (\n",
    "        _remove_ext(v).replace(w, '') + '2d' + _get_ext(v)\n",
    "        if v.startswith(w)\n",
    "        else v\n",
    "    ),\n",
    "    lambda v, w='Volumetric': (\n",
    "        _remove_ext(v).replace(w, '') + '3d' + _get_ext(v)\n",
    "        if v.startswith(w)\n",
    "        else v\n",
    "    ),\n",
    "]\n",
    "\n",
    "RULES = [] + RULES_NAME\n",
    "\n",
    "\n",
    "def apply_rules(rules, text):\n",
    "    _fn = text\n",
    "    for r in rules:\n",
    "        _fn = r(_fn)\n",
    "    return _fn\n",
    "\n",
    "\n",
    "def convert_filenames(filenames, extra_rules: list = []):\n",
    "    rules = RULES + extra_rules\n",
    "    \n",
    "    result = []\n",
    "    for fn in filenames:\n",
    "        result.append(apply_rules(rules, fn))\n",
    "    return result\n",
    "\n",
    "\n",
    "# test\n",
    "at_cuda_files = convert_filenames(thcunn_files, [lambda v: v.replace('Sampling', 'Sample')])\n",
    "at_cuda_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xmn/dev/quansight/tmp/pytorch/output\n",
      "total 92K\n",
      "drwxrwxr-x 2 xmn xmn 4,0K abr 17 21:05 .\n",
      "drwxrwxr-x 3 xmn xmn 4,0K abr 17 20:36 ..\n",
      "-rw-rw-r-- 1 xmn xmn  11K abr 17 22:18 UpSampleBicubic2d.cu\n",
      "-rw-rw-r-- 1 xmn xmn  11K abr 17 22:18 UpSampleBilinear2d.cu\n",
      "-rw-rw-r-- 1 xmn xmn 8,6K abr 17 22:18 UpSampleLinear1d.cu\n",
      "-rw-rw-r-- 1 xmn xmn 7,4K abr 17 22:18 UpSampleNearest1d.cu\n",
      "-rw-rw-r-- 1 xmn xmn 8,6K abr 17 22:18 UpSampleNearest2d.cu\n",
      "-rw-rw-r-- 1 xmn xmn 9,9K abr 17 22:18 UpSampleNearest3d.cu\n",
      "-rw-rw-r-- 1 xmn xmn  14K abr 17 22:18 UpSampleTrilinear3d.cu\n"
     ]
    }
   ],
   "source": [
    "def create_aten_cuda_files(\n",
    "    output_path: str,\n",
    "    thcunn_path: str,\n",
    "    at_cuda_path: str,\n",
    "    th_at_filenames: list\n",
    "): \n",
    "    \"\"\"Porting code from `/aten/src/THCUNN/generic` and `/aten/src/THCUNN`\n",
    "    to `/aten/src/ATen/native/cuda/`\n",
    "    \n",
    "    \"\"\"\n",
    "    for th_fn, at_fn in th_at_filenames:\n",
    "        # get file data from THCUNN\n",
    "        path_src = os.path.join(thcunn_path, th_fn)\n",
    "        at_file_output_path = os.path.join(output_path, at_fn)\n",
    "        # copy also properties and metadata\n",
    "        shutil.copy2(path_src, at_file_output_path)\n",
    "        # write output file\n",
    "        with open(at_file_output_path, 'a') as f_dst:\n",
    "            # get file data from THCUNN/generic\n",
    "            f_dst.write('\\n')\n",
    "            path_src = os.path.join(thcunn_path, 'generic', th_fn) \n",
    "            with open(path_src, 'r') as f_src:\n",
    "                f_dst.write('// THCUNN/generic ')\n",
    "                f_dst.write(f_src.read())\n",
    "            \n",
    "            # get file data from ATen/native/cuda\n",
    "            f_dst.write('\\n')\n",
    "            path_src = os.path.join(at_cuda_path, at_fn)\n",
    "            with open(path_src, 'r') as f_src:\n",
    "                f_dst.write('// ATen/native/cuda ')\n",
    "                f_dst.write(f_src.read())\n",
    "\n",
    "# test\n",
    "create_aten_cuda_files(\n",
    "    output_path, \n",
    "    thcunn_path,\n",
    "    at_cuda_path,\n",
    "    zip(thcunn_files, at_cuda_files)\n",
    ")\n",
    "\n",
    "print(output_path)\n",
    "!ls -lah {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_th2at(files_path: list):\n",
    "    def add_replace_rule(by, to):\n",
    "        return lambda v: v.replace(by, to)\n",
    "    \n",
    "    rules = [\n",
    "        add_replace_rule(' int ', ' int64_t '),\n",
    "        add_replace_rule('Acctype', 'accscalar_t'),\n",
    "        add_replace_rule('Dtype', 'scalar_t'),\n",
    "        add_replace_rule(\n",
    "            'ScalarConvert<Dtype, Acctype>::to',\n",
    "            'static_cast<accscalar_t>'\n",
    "        ), \n",
    "        add_replace_rule('output.getSize', 'output.size'),\n",
    "        add_replace_rule(\n",
    "            'THCNumerics<Dtype>::min()',\n",
    "            'at::numeric_lmits<scalar_t>::lowest()'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for f_path in files_path:\n",
    "        with open(f_path, 'r') as f:\n",
    "            f_content = f.read()\n",
    "        \n",
    "        for rule in rules:\n",
    "            f_content = rule(f_content)\n",
    "            \n",
    "        with open(f_path, 'w') as f:\n",
    "            f.write(f_content)\n",
    "\n",
    "cuda_th2at([\n",
    "    os.path.join(output_path, fn) \n",
    "    for fn in at_cuda_files\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THNN_(SpatialUpSamplingBicubic_shapeCheck) UpSamplingBicubic_shapeCheck2d\n",
      "THNN_(SpatialUpSamplingBicubic_updateOutput) UpSamplingBicubic_updateOutput2d\n",
      "THNN_(SpatialUpSamplingBicubic_shapeCheck) UpSamplingBicubic_shapeCheck2d\n",
      "THNN_(SpatialUpSamplingBicubic_updateGradInput) UpSamplingBicubic_updateGradInput2d\n",
      "THNN_(SpatialUpSamplingBicubic_shapeCheck) UpSamplingBicubic_shapeCheck2d\n"
     ]
    }
   ],
   "source": [
    "# experimental\n",
    "with open(os.path.join(output_path, at_cuda_files[0]), 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "result = re.finditer('THNN_\\((.*)\\)', text, re.MULTILINE)\n",
    "for r in result:\n",
    "    print(r.group(0), apply_rules(RULES_NAME, r.group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <THCUNN/THCUNN.h>\n",
      "#include <THC/THCTensor.hpp>\n",
      "#include <THCUNN/common.h>\n",
      "#include <THCUNN/upsampling.h>\n",
      "#include <THC/THCDeviceTensor.cuh>\n",
      "#include <THC/THCDeviceTensorUtils.cuh>\n",
      "#include <THC/THCDeviceUtils.cuh>\n",
      "#include <TH/THHalf.h>\n",
      "#include <THCUNN/THCHalfAutoNumerics.cuh>\n",
      "#include <THC/THCAtomics.cuh>\n",
      "\n",
      "template<typename scalar_t, typename accscalar_t>\n",
      "#if defined(__HIP_PLATFORM_HCC__)\n",
      "__launch_bounds__(1024)\n",
      "#endif\n",
      "__global__ void bicubic_interp2d_kernel(\n",
      "  const int64_t num_elements,\n",
      "  const accscalar_t height_scale,\n",
      "  const accscalar_t width_scale,\n",
      "  const THCDeviceTensor<scalar_t, 4> in_data,\n",
      "  THCDeviceTensor<scalar_t, 4> out_data\n",
      ") {\n",
      "\n",
      "  int64_t index = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "  const int64_t batchsize = in_data.getSize(0);\n",
      "  const int64_t channels = in_data.getSize(1);\n",
      "  const int64_t input_height = in_data.getSize(2);\n",
      "  const int64_t input_width = in_data.getSize(3);\n",
      "  const int64_t output_height = out_data.getSize(2);\n",
      "  const int64_t output_width = out_data.getSize(3);\n",
      "\n",
      "  if (index >= num_elements) {\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  // Special case: input and output are the same size, just copy\n",
      "  const int64_t output_x = index % output_width;\n",
      "  const int64_t output_y = index / output_width;\n",
      "  if (input_height == output_height && input_width == output_width) {\n",
      "    for (int n = 0; n < batchsize; n++){\n",
      "      for (int c = 0; c < channels; c++) {\n",
      "        const scalar_t val = in_data[n][c][output_y][output_x];\n",
      "        out_data[n][c][output_x][output_y] = val;\n",
      "      }\n",
      "    }\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  // Interpolation kernel\n",
      "  accscalar_t real_x = width_scale * output_x;\n",
      "  int64_t in_x = real_x;\n",
      "  accscalar_t t_x = real_x - in_x;\n",
      "\n",
      "  accscalar_t real_y = height_scale * output_y;\n",
      "  int64_t in_y = real_y;\n",
      "  accscalar_t t_y = real_y - in_y;\n",
      "\n",
      "  for (int n = 0; n < batchsize ; n++) {\n",
      "    for (int c = 0; c < channels; c++) {\n",
      "      accscalar_t coefficients[4];\n",
      "\n",
      "      for (int k = 0; k < 4; k++) {\n",
      "        coefficients[k] = cubic_interp1d(\n",
      "          upsampling_get_value_bounded<scalar_t>(\n",
      "            in_data, c, n, input_width, input_height, in_x - 1, in_y - 1 + k),\n",
      "          upsampling_get_value_bounded<scalar_t>(\n",
      "            in_data, c, n, input_width, input_height, in_x + 0, in_y - 1 + k),\n",
      "          upsampling_get_value_bounded<scalar_t>(\n",
      "            in_data, c, n, input_width, input_height, in_x + 1, in_y - 1 + k),\n",
      "          upsampling_get_value_bounded<scalar_t>(\n",
      "            in_data, c, n, input_width, input_height, in_x + 2, in_y - 1 + k),\n",
      "          t_x\n",
      "        );\n",
      "      }\n",
      "\n",
      "      out_data[n][c][output_y][output_x] = ScalarConvert<accscalar_t, scalar_t>::to(cubic_interp1d(\n",
      "        coefficients[0],\n",
      "        coefficients[1],\n",
      "        coefficients[2],\n",
      "        coefficients[3],\n",
      "        t_y\n",
      "      ));\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "// Backward (adjoint) operation 1 <- 2 (accumulates)\n",
      "template <typename scalar_t, typename accscalar_t>\n",
      "#if defined(__HIP_PLATFORM_HCC__)\n",
      "__launch_bounds__(1024)\n",
      "#endif\n",
      "__global__ void bicubic_interp2d_backward_kernel(\n",
      "  const int64_t num_elements,\n",
      "  const accscalar_t height_scale,\n",
      "  const accscalar_t width_scale,\n",
      "  const bool align_corners,\n",
      "  THCDeviceTensor<scalar_t, 4> in_data,\n",
      "  const THCDeviceTensor<scalar_t, 4> out_data\n",
      "){\n",
      "\n",
      "  int64_t index = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "  const int64_t batchsize = in_data.getSize(0);\n",
      "  const int64_t channels = in_data.getSize(1);\n",
      "  const int64_t input_height = in_data.getSize(2);\n",
      "  const int64_t input_width = in_data.getSize(3);\n",
      "  const int64_t output_height = out_data.getSize(2);\n",
      "  const int64_t output_width = out_data.getSize(3);\n",
      "\n",
      "  if (index >= num_elements) {\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  const int64_t output_x = index % output_width;\n",
      "  const int64_t output_y = index / output_width;\n",
      "  // special case: output_xust copy\n",
      "  if (input_height == output_height && input_width == output_width) {\n",
      "    for (int n = 0; n < batchsize ; n++){\n",
      "      for (int c = 0; c < channels; ++c) {\n",
      "        const scalar_t val = out_data[n][c][output_y][output_x];\n",
      "        in_data[n][c][output_y][output_x] += val;\n",
      "      }\n",
      "    }\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  accscalar_t real_x = width_scale * output_x;\n",
      "  int64_t input_x = real_x;\n",
      "  accscalar_t t_x = real_x - input_x;\n",
      "\n",
      "  accscalar_t real_y = height_scale * output_y;\n",
      "  int64_t input_y = real_y;\n",
      "  accscalar_t t_y = real_y - input_y;\n",
      "\n",
      "  accscalar_t x_coeffs[4];\n",
      "  accscalar_t y_coeffs[4];\n",
      "\n",
      "  get_cubic_upsampling_coefficients(x_coeffs, t_x);\n",
      "  get_cubic_upsampling_coefficients(y_coeffs, t_y);\n",
      "\n",
      "  for (int n = 0; n < batchsize ; n++){\n",
      "    for (int c = 0; c < channels; ++c) {\n",
      "      scalar_t out_value = out_data[n][c][output_y][output_x];\n",
      "      for (int i = 0; i < 4; i++) {\n",
      "        for (int j = 0; j < 4; j++) {\n",
      "          upsampling_increment_value_bounded<scalar_t, accscalar_t>(\n",
      "            in_data,\n",
      "            c,\n",
      "            n,\n",
      "            input_width,\n",
      "            input_height,\n",
      "            input_x - 1 + j,\n",
      "            input_y - 1 + i,\n",
      "            out_value * y_coeffs[i] * x_coeffs[j]\n",
      "          );\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "#include <THCUNN/generic/SpatialUpSamplingBicubic.cu>\n",
      "#include <THC/THCGenerateFloatTypes.h>\n",
      "\n",
      "// THCUNN/generic #ifndef THC_GENERIC_FILE\n",
      "#define THC_GENERIC_FILE \"THCUNN/generic/SpatialUpSamplingBicubic.cu\"\n",
      "#else\n",
      "\n",
      "#include <THCUNN/upsampling.h>\n",
      "#include <ATen/cuda/CUDAContext.h>\n",
      "\n",
      "static inline void THNN_(SpatialUpSamplingBicubic_shapeCheck)\n",
      "                        (THCState *state,\n",
      "                         THCTensor *input, THCTensor *gradOutput,\n",
      "                         int64_t nBatch, int64_t nChannels,\n",
      "                         int64_t inputHeight, int64_t inputWidth,\n",
      "                         int64_t outputHeight, int64_t outputWidth) {\n",
      "  THArgCheck(inputHeight > 0 && inputWidth > 0\n",
      "             && outputHeight > 0 && outputWidth > 0, 2,\n",
      "             \"input and output sizes should be greater than 0,\"\n",
      "             \" but got input (H: %d, W: %d) output (H: %d, W: %d)\",\n",
      "             inputHeight, inputWidth, outputHeight, outputWidth);\n",
      "  if (input != NULL) {\n",
      "     THCUNN_argCheck(state, !input->is_empty() && input->dim() == 4, 2, input,\n",
      "                     \"non-empty 4D input tensor expected but got: %s\");\n",
      "  }\n",
      "\n",
      "  if (gradOutput != NULL) {\n",
      "    THCUNN_check_dim_size(state, gradOutput, 4, 0, nBatch);\n",
      "    THCUNN_check_dim_size(state, gradOutput, 4, 1, nChannels);\n",
      "    THCUNN_check_dim_size(state, gradOutput, 4, 2, outputHeight);\n",
      "    THCUNN_check_dim_size(state, gradOutput, 4, 3, outputWidth);\n",
      "  }\n",
      "}\n",
      "\n",
      "void THNN_(SpatialUpSamplingBicubic_updateOutput)(\n",
      "           THCState *state,\n",
      "           THCTensor *input,\n",
      "           THCTensor *output,\n",
      "           int64_t outputHeight,\n",
      "           int64_t outputWidth,\n",
      "           bool align_corners)\n",
      "{\n",
      "  int64_t nbatch = THCTensor_(size)(state, input, 0);\n",
      "  int64_t channels = THCTensor_(size)(state, input, 1);\n",
      "  int64_t inputHeight = THCTensor_(size)(state, input, 2);\n",
      "  int64_t inputWidth = THCTensor_(size)(state, input, 3);\n",
      "  THNN_(SpatialUpSamplingBicubic_shapeCheck)\n",
      "       (state, input, NULL,\n",
      "        nbatch, channels,\n",
      "        inputHeight, inputWidth,\n",
      "        outputHeight, outputWidth);\n",
      "\n",
      "  THCUNN_assertSameGPU(state, 2, input, output);\n",
      "  THCTensor_(resize4d)(state, output,\n",
      "                       THCTensor_(size)(state, input, 0),\n",
      "                       THCTensor_(size)(state, input, 1),\n",
      "                       outputHeight, outputWidth);\n",
      "  THCTensor_(zero)(state, output);\n",
      "  THCDeviceTensor<scalar_t, 4> idata = toDeviceTensor<scalar_t, 4>(state, input);\n",
      "  THCDeviceTensor<scalar_t, 4> odata = toDeviceTensor<scalar_t, 4>(state, output);\n",
      "  THAssert(inputHeight > 0 && inputWidth > 0 && outputHeight > 0 && outputWidth > 0);\n",
      "\n",
      "  // Get scaling factors\n",
      "  const accreal rheight = linear_upsampling_compute_scale<accreal>(inputHeight, outputHeight, align_corners);\n",
      "  const accreal rwidth = linear_upsampling_compute_scale<accreal>(inputWidth, outputWidth, align_corners);\n",
      "\n",
      "  const int64_t num_output_elements = outputHeight * outputWidth;\n",
      "  const int64_t max_threads =\n",
      "    at::cuda::getCurrentDeviceProperties()->maxThreadsPerBlock;\n",
      "\n",
      "  // Launch kernel\n",
      "  cudaStream_t stream = THCState_getCurrentStream(state);\n",
      "  bicubic_interp2d_kernel<scalar_t, accreal> <<<\n",
      "    THCCeilDiv(num_output_elements, max_threads),\n",
      "    max_threads,\n",
      "    0,\n",
      "    stream\n",
      "  >>>(num_output_elements, rheight, rwidth, idata, odata);\n",
      "  THCudaCheck(cudaGetLastError());\n",
      "}\n",
      "\n",
      "\n",
      "void THNN_(SpatialUpSamplingBicubic_updateGradInput)(\n",
      "           THCState *state,\n",
      "           THCTensor *gradOutput,\n",
      "           THCTensor *gradInput,\n",
      "           int64_t nbatch,\n",
      "           int64_t nchannels,\n",
      "           int64_t inputHeight,\n",
      "           int64_t inputWidth,\n",
      "           int64_t outputHeight,\n",
      "           int64_t outputWidth,\n",
      "           bool align_corners)\n",
      "{\n",
      "  THNN_(SpatialUpSamplingBicubic_shapeCheck)\n",
      "       (state, NULL, gradOutput,\n",
      "        nbatch, nchannels,\n",
      "        inputHeight, inputWidth,\n",
      "        outputHeight, outputWidth);\n",
      "  gradOutput = THCTensor_(newContiguous)(state, gradOutput);\n",
      "  THCUNN_assertSameGPU(state, 2, gradOutput, gradInput);\n",
      "  THCTensor_(resize4d)(state, gradInput, nbatch, nchannels, inputHeight, inputWidth);\n",
      "  THCTensor_(zero)(state, gradInput);\n",
      "  THCDeviceTensor<scalar_t, 4> in_data = toDeviceTensor<scalar_t, 4>(state, gradInput);\n",
      "  THCDeviceTensor<scalar_t, 4> out_data = toDeviceTensor<scalar_t, 4>(state, gradOutput);\n",
      "  const accreal rheight = linear_upsampling_compute_scale<accreal>(inputHeight, outputHeight, align_corners);\n",
      "  const accreal rwidth = linear_upsampling_compute_scale<accreal>(inputWidth, outputWidth, align_corners);\n",
      "  const int64_t num_kernels = outputHeight * outputWidth;\n",
      "  const int64_t num_threads =\n",
      "    at::cuda::getCurrentDeviceProperties()->maxThreadsPerBlock;\n",
      "  cudaStream_t stream = THCState_getCurrentStream(state);\n",
      "  bicubic_interp2d_backward_kernel<scalar_t ,accreal> <<<THCCeilDiv(num_kernels, num_threads),\n",
      "  num_threads, 0, stream>>>(num_kernels, rheight, rwidth, align_corners, in_data, out_data);\n",
      "  THCudaCheck(cudaGetLastError());\n",
      "  THCTensor_(free)(state, gradOutput);\n",
      "}\n",
      "\n",
      "#endif\n",
      "\n",
      "// ATen/native/cuda #include <ATen/ATen.h>\n",
      "#include <ATen/NativeFunctions.h>\n",
      "#include <ATen/LegacyTHFunctions.h>\n",
      "\n",
      "namespace at {\n",
      "namespace native {\n",
      "\n",
      "Tensor& upsample_bicubic2d_out_cuda(\n",
      "    Tensor& output,\n",
      "    const Tensor& input,\n",
      "    IntArrayRef output_size,\n",
      "    bool align_corners) {\n",
      "    return at::legacy::th::_thnn_upsample_bicubic2d_forward_out(\n",
      "        output, input, output_size, align_corners);\n",
      "}\n",
      "\n",
      "Tensor upsample_bicubic2d_cuda(\n",
      "    const Tensor& input,\n",
      "    IntArrayRef output_size,\n",
      "    bool align_corners) {\n",
      "    return at::legacy::th::_thnn_upsample_bicubic2d_forward(\n",
      "        input, output_size, align_corners);\n",
      "}\n",
      "\n",
      "Tensor& upsample_bicubic2d_backward_out_cuda(\n",
      "    Tensor& grad_input,\n",
      "    const Tensor& grad_output,\n",
      "    IntArrayRef output_size,\n",
      "    IntArrayRef input_size,\n",
      "    bool align_corners) {\n",
      "    return at::legacy::th::_thnn_upsample_bicubic2d_backward_out(\n",
      "        grad_input, grad_output, output_size, input_size, align_corners);\n",
      "}\n",
      "\n",
      "Tensor upsample_bicubic2d_backward_cuda(\n",
      "    const Tensor& grad_output,\n",
      "    IntArrayRef output_size,\n",
      "    IntArrayRef input_size,\n",
      "    bool align_corners) {\n",
      "    return at::legacy::th::_thnn_upsample_bicubic2d_backward(\n",
      "        grad_output, output_size, input_size, align_corners);\n",
      "}\n",
      "\n",
      "} // native\n",
      "} // at\n"
     ]
    }
   ],
   "source": [
    "!cat {output_path}/{aten_cuda_files[0]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
