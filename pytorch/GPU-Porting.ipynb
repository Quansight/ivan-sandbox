{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Porting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually check:\n",
    "\n",
    "* `AT_CHECK` just have 2 args: condition and message\n",
    "* Check order of height, width in parameters of functions such as: resize,  \n",
    "\n",
    "## Warnings:\n",
    " \n",
    "* Missing THCUNN_check_dim_size\n",
    "* not garanteed for nested parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = os.environ['HOME']\n",
    "output_path = '/tmp/pytorch/output'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "pytorch_path = os.path.join(\n",
    "    home_path,\n",
    "    'dev/quansight/pytorch-project/pytorch'\n",
    ")\n",
    "thcunn_path = os.path.join(pytorch_path, 'aten/src/THCUNN')\n",
    "at_cuda_path = os.path.join(pytorch_path, 'aten/src/ATen/native/cuda')\n",
    "\n",
    "thcunn_files = [\n",
    "    'Im2Col.cu',\n",
    "]\n",
    "thcunn_h_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove `aten/src/THNN/generic/*.c` files that is being ported\n",
    "# Remove functions to be ported from:\n",
    "# `/aten/src/THCUNN/CMakeLists.txt`\n",
    "# `/aten/src/THCUNN/generic/THCUNN.h`\n",
    "# `/aten/src/THNN/init.cpp`\n",
    "# `/aten/src/ATen/nn.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Im2Col.cu']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _remove_ext(v):\n",
    "    if '.' in v:\n",
    "        return v.split('.')[0]\n",
    "    return v\n",
    "\n",
    "def _get_ext(v):\n",
    "    if '.' in v:\n",
    "        return '.' + v.split('.')[-1]\n",
    "    return ''\n",
    "    \n",
    "RULES_NAME = [\n",
    "    lambda v, w='Temporal': (\n",
    "        _remove_ext(v).replace(w, '') + '1d' + _get_ext(v)\n",
    "        if v.startswith(w)\n",
    "        else v\n",
    "    ),\n",
    "    lambda v, w='Spatial': (\n",
    "        _remove_ext(v).replace(w, '') + '2d' + _get_ext(v)\n",
    "        if v.startswith(w)\n",
    "        else v\n",
    "    ),\n",
    "    lambda v, w='Volumetric': (\n",
    "        _remove_ext(v).replace(w, '') + '3d' + _get_ext(v)\n",
    "        if v.startswith(w)\n",
    "        else v\n",
    "    ),\n",
    "]\n",
    "\n",
    "RULES_NAME_EXTRA = RULES_NAME + [\n",
    "    lambda v: v.replace('Sampling', 'Sample')\n",
    "]\n",
    "\n",
    "RULES = [] + RULES_NAME_EXTRA\n",
    "\n",
    "\n",
    "def apply_rules(rules, text):\n",
    "    _fn = text\n",
    "    for r in rules:\n",
    "        _fn = r(_fn)\n",
    "    return _fn\n",
    "\n",
    "\n",
    "def convert_filenames(filenames, extra_rules: list = []):\n",
    "    rules = RULES + extra_rules\n",
    "    \n",
    "    result = []\n",
    "    for fn in filenames:\n",
    "        result.append(apply_rules(rules, fn))\n",
    "    return result\n",
    "\n",
    "\n",
    "# test\n",
    "at_cuda_files = convert_filenames(thcunn_files)\n",
    "at_cuda_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aten_cuda_files(\n",
    "    output_path: str,\n",
    "    thcunn_path: str,\n",
    "    at_cuda_path: str,\n",
    "    th_at_filenames: list,\n",
    "    just_gpu_porting: bool = True\n",
    "): \n",
    "    \"\"\"Porting code from `/aten/src/THCUNN/generic` and `/aten/src/THCUNN`\n",
    "    to `/aten/src/ATen/native/cuda/`\n",
    "    \n",
    "    \"\"\"\n",
    "    for th_fn, at_fn in th_at_filenames:\n",
    "        # get file data from THCUNN\n",
    "        path_src = os.path.join(thcunn_path, th_fn)\n",
    "        at_file_output_path = os.path.join(output_path, at_fn)\n",
    "        # copy also properties and metadata\n",
    "        shutil.copy2(path_src, at_file_output_path)\n",
    "        # write output file\n",
    "        with open(at_file_output_path, 'a') as f_dst:\n",
    "            # get file data from THCUNN/generic\n",
    "            f_dst.write('\\n')\n",
    "            path_src = os.path.join(thcunn_path, 'generic', th_fn) \n",
    "            with open(path_src, 'r') as f_src:\n",
    "                f_dst.write('\\n// THCUNN/generic\\n')\n",
    "                f_dst.write(f_src.read())\n",
    "            \n",
    "            # get file data from ATen/native/cuda\n",
    "            # expetec a initial gpu porting after a `just cpu porting`\n",
    "            if just_gpu_porting:\n",
    "                f_dst.write('\\n')\n",
    "                path_src = os.path.join(at_cuda_path, at_fn)\n",
    "                if os.path.isfile(path_src):\n",
    "                    with open(path_src, 'r') as f_src:\n",
    "                        f_dst.write('\\n// ATen/native/cuda\\n')\n",
    "                        f_dst.write(f_src.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/pytorch/output\n",
      "total 16K\n",
      "drwxrwxr-x 2 xmn xmn 4,0K may 14 18:58 .\n",
      "drwxrwxr-x 3 xmn xmn 4,0K may 14 18:57 ..\n",
      "-rw-rw-r-- 1 xmn xmn 4,8K may 14 19:00 Im2Col.cu\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "create_aten_cuda_files(\n",
    "    output_path, \n",
    "    thcunn_path,\n",
    "    at_cuda_path,\n",
    "    zip(thcunn_files, at_cuda_files)\n",
    ")\n",
    "\n",
    "print(output_path)\n",
    "!ls -lah {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_replace_rule(by, to):\n",
    "    return lambda v: v.replace(by, to)\n",
    "\n",
    "\n",
    "def th2at(text: str, extra_rules: list = []):\n",
    "    # replace rules\n",
    "    rules = [\n",
    "        ('#include <THCUNN/THCUNN.h>', \n",
    "         '/* TODO: remove duplicated includes */\\n'\n",
    "         '#include <ATen/ATen.h>\\n'\n",
    "         '#include <ATen/AccumulateType.h>\\n'\n",
    "         '#include <ATen/NativeFunctions.h>\\n'\n",
    "         '#include <ATen/TensorUtils.h>\\n'\n",
    "         '#include <ATen/Utils.h>\\n'\n",
    "         '#include <ATen/cuda/CUDAContext.h>\\n'\n",
    "         '#include <ATen/cuda/CUDAApplyUtils.cuh>\\n'\n",
    "        ),\n",
    "        ('getSize(', 'size('),\n",
    "        ('Acctype', 'accscalar_t'),\n",
    "        ('Dtype', 'scalar_t'),\n",
    "        ('ScalarConvert<scalar_t, accscalar_t>::to',\n",
    "         'static_cast<accscalar_t>'),\n",
    "        ('ScalarConvert<accscalar_t, scalar_t>::to',\n",
    "         'static_cast<scalar_t>'),\n",
    "        ('THCNumerics<scalar_t>::min()',\n",
    "         'at::numeric_lmits<scalar_t>::lowest()'),\n",
    "        ('THCUNN_argCheck', '/* TODO: AT_CHECK just have 2 args*/ AT_CHECK'),\n",
    "        ('THAssert', 'AT_ASSERT'),\n",
    "        ('THCTensor ', 'Tensor '),\n",
    "        ('THCTensor*', 'Tensor*'),\n",
    "        ('putDepth', 'put_depth'),\n",
    "        ('putHeight', 'put_height'),\n",
    "        ('putWidth', 'put_width'),\n",
    "        ('gradOut', 'grad_out'),\n",
    "        ('gradIn', 'grad_in'),\n",
    "        ('nBatch', 'nbatch'),\n",
    "        ('nChannel', 'nchannel'),\n",
    "        ('THCState *state,', ''),\n",
    "        ('THCDeviceTensor', 'PackedTensorAccessor'),\n",
    "        ('state, ', ''),\n",
    "        ('THCState_getCurrentStream(state)', 'at::cuda::getCurrentCUDAStream()'),\n",
    "        ('THArgCheck(', 'AT_CHECK('),\n",
    "        ('THCudaCheck(cudaGetLastError())',\n",
    "         'AT_CUDA_CHECK(cudaGetLastError())'),\n",
    "        ('NULL,', 'Tensor(),'),\n",
    "        ('THCNumerics<scalar_t>::min()', 'at::numeric_limits<scalar_t>::lowest()'),\n",
    "        ('->dim()', '.dim()'),\n",
    "        ('->size(', '.size('),\n",
    "        ('THCeilDiv', 'cuda::ATenCeilDiv'),\n",
    "        ('nInput', 'n_input'),\n",
    "        ('nOutput', 'n_output'),\n",
    "        ('putPlane', 'put_plane'),\n",
    "        ('THCTensor_(new)(state)', 'Tensor()'),\n",
    "        ('batchSize', 'batch_size')\n",
    "    ] + extra_rules\n",
    "    \n",
    "    for by, to in rules:\n",
    "        text = text.replace(by, to)\n",
    "        \n",
    "    # regex rules\n",
    "    # TODO:\n",
    "    # - toDeviceTensor\n",
    "    rules = (\n",
    "        # rule, output pattern \n",
    "        (r'THNN_\\((.*)\\)', None),\n",
    "        (r'THCTensor_\\(size\\)\\(\\s*([^,]*),\\s*(.*)\\s*\\)', '{}.size({})'),\n",
    "        (r'THCTensor_\\(resize([0-9]*)d\\)\\(\\s*([^,]*),\\s*(.*)\\s*\\)', '{1}.resize_({{ {2} }})'),\n",
    "        (r'THCTensor_\\(nDimensionLegacyNoScalars\\)\\(\\s*(.*)\\s*\\)', '{}.ndimension()'),\n",
    "        (r'THCTensor_\\(zero\\)\\(\\s*(.*)\\s*\\)', '{0}.zero_()'),\n",
    "        (r'THCTensor_\\(data\\)\\(\\s*(.*)\\s*\\)', '{0}.data()'),\n",
    "        (r'[!](.*)->is_empty\\(\\)', '{}.numel() != 0'),\n",
    "        (r'(\\w)\\s*!=\\s*NULL', '{}.defined()'),\n",
    "        (r'THCUNN_assertSameGPU\\([0-9]*,\\s*(.*)\\s*\\);', \n",
    "         '/* TODO: TensorArg tensorname_arg{{tensorname, \"tensorname\", 1}}; */\\n'\n",
    "         '/* TODO: checkAllSameGPU should use TensorArg */\\n'\n",
    "         'checkAllSameGPU(\\n'\n",
    "         '  \"/* TODO: use the name of the function as description here */\",'\n",
    "         '  {{ {} }});'), \n",
    "        (r'(.*)=\\s*THCTensor_\\(newContiguous\\)\\(\\s*(.*)\\s*\\);', \n",
    "         'Tensor {0} = {1}_.contiguous(); /* TODO: add _ to the arg definition above */'),\n",
    "        (r'accscalar_t\\(\\s*(.*)\\s*\\)', 'static_cast<accscalar_t>({})'),\n",
    "        (r'THCNumerics\\<scalar_t\\>::ne\\(\\s*(.*),\\s*(.*)\\s*\\)\\s*', '{} != {}'),\n",
    "    )\n",
    "    \n",
    "    for rule, output_format in rules:\n",
    "        result = re.finditer(rule, text, re.MULTILINE)\n",
    "        for r in result:\n",
    "            _in = r.group(0)\n",
    "            if output_format is None:\n",
    "                _out = r.group(1)\n",
    "            else:\n",
    "                _out = output_format.format(*r.groups())\n",
    "            text = text.replace(_in, apply_rules(RULES_NAME_EXTRA, _out))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cuda_th2at(files_path: list, extra_rules: list = []):\n",
    "    for f_path in files_path:\n",
    "        with open(f_path, 'r') as f:\n",
    "            f_content = th2at(f.read(), extra_rules)\n",
    "        \n",
    "        with open(f_path, 'w') as f:\n",
    "            f.write(f_content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* TODO: remove duplicated includes */\n",
      "#include <ATen/ATen.h>\n",
      "#include <ATen/AccumulateType.h>\n",
      "#include <ATen/NativeFunctions.h>\n",
      "#include <ATen/TensorUtils.h>\n",
      "#include <ATen/Utils.h>\n",
      "#include <ATen/cuda/CUDAContext.h>\n",
      "#include <ATen/cuda/CUDAApplyUtils.cuh>\n",
      "\n",
      "#include <THCUNN/common.h>\n",
      "#include <ATen/cuda/Im2Col.h>\n",
      "\n",
      "#include <TH/THHalf.h>\n",
      "#include <THCUNN/THCHalfAutoNumerics.cuh>\n",
      "#include <THC/THCTensor.hpp>\n",
      "#include <THC/THCStorage.hpp>\n",
      "\n",
      "#include <THCUNN/generic/Im2Col.cu>\n",
      "#include <THC/THCGenerateFloatTypes.h>\n",
      "\n",
      "\n",
      "// THCUNN/generic\n",
      "#ifndef THC_GENERIC_FILE\n",
      "#define THC_GENERIC_FILE \"THCUNN/generic/Im2Col.cu\"\n",
      "#else\n",
      "\n",
      "#include <ATen/div_rtn.h>\n",
      "\n",
      "static inline void Im2Col_shapeCheck(\n",
      "                         \n",
      "                         Tensor *input,\n",
      "                         Tensor *grad_output,\n",
      "                         int64_t kH, int64_t kW, int64_t dH, int64_t dW,\n",
      "                         int64_t padH, int64_t padW, int64_t sH, int64_t sW) {\n",
      "\n",
      "  AT_CHECK(kW > 0 && kH > 0, 4,\n",
      "             \"kernel size should be greater than zero, but got kH: %d kW: %d\", kH, kW);\n",
      "  AT_CHECK(dW > 0 && dH > 0, 6,\n",
      "             \"dilation should be greater than zero, but got dH: %d dW: %d\", dH, dW);\n",
      "  AT_CHECK(padW >= 0 && padH >= 0, 8,\n",
      "             \"padding should be non-negative, but got padH: %d padW: %d\", padH, padW);\n",
      "  AT_CHECK(sW > 0 && sH > 0, 10,\n",
      "             \"stride should be greater than zero, but got sH: %d sW: %d\", sH, sW);\n",
      "\n",
      "  int64_t ndim = input.ndimension();\n",
      "  /* TODO: AT_CHECK just have 2 args*/ AT_CHECK(input.numel() != 0 && (ndim == 3 || ndim == 4), 2, input,\n",
      "                \"Expected non-empty 3D or 4D input tensor, but got input of shape %s\");\n",
      "\n",
      "  int dim_batch = 0;\n",
      "  if (ndim == 3) {\n",
      "    dim_batch = -1;\n",
      "  }\n",
      "  int64_t n_input_plane  = input.size(dim_batch + 1);\n",
      "  int64_t input_height  = input.size(dim_batch + 2);\n",
      "  int64_t input_width   = input.size(dim_batch + 3);\n",
      "  int64_t output_height = div_rtn<int64_t>(input_height + 2 * padH - (dH * (kH - 1) + 1),  sH) + 1;\n",
      "  int64_t output_width  = div_rtn<int64_t>(input_width + 2 * padW - (dW * (kW - 1) + 1), sW) + 1;\n",
      "\n",
      "  if (output_height < 1 || output_width < 1) {\n",
      "    THError(\"Given input with spatial size (%d, %d), kernel_size=(%d, %d), \"\n",
      "            \"dilation=(%d, %d), padding=(%d, %d), calculated \"\n",
      "            \"shape of the array of sliding blocks as (%d, %d), which is \"\n",
      "            \"too small (non-positive).\",\n",
      "            input_height, input_height, kH, kW, dH, dW, padH, padW,\n",
      "            output_height, output_width);\n",
      "  }\n",
      "}\n",
      "\n",
      "void Im2Col_updateOutput(\n",
      "           \n",
      "           Tensor *input,\n",
      "           Tensor *output,\n",
      "           int64_t kH, int64_t kW,\n",
      "           int64_t dH, int64_t dW,\n",
      "           int64_t padH, int64_t padW,\n",
      "           int64_t sH, int64_t sW) {\n",
      "\n",
      "  /* TODO: TensorArg tensorname_arg{tensorname, \"tensorname\", 1}; */\n",
      "/* TODO: checkAllSameGPU should use TensorArg */\n",
      "checkAllSameGPU(\n",
      "  \"/* TODO: use the name of the function as description here */\",  { input, output });\n",
      "\n",
      "  Im2Col_shapeCheck(input, Tensor(), kH, kW, dH, dW, padH, padW, sH, sW);\n",
      "\n",
      "Tensor   input  = input_.contiguous(); /* TODO: add _ to the arg definition above */\n",
      "  bool batched_input = true;\n",
      "  if (input.dim() == 3) {\n",
      "    batched_input = false;\n",
      "    input.resize_({ 1, input.size(0), input.size(1), input.size(2) });\n",
      "  }\n",
      "\n",
      "  int64_t batch_size    = input.size(0);\n",
      "  int64_t n_input_plane  = input.size(1);\n",
      "  int64_t input_height  = input.size(2);\n",
      "  int64_t input_width   = input.size(3);\n",
      "\n",
      "  int64_t output_height = (input_height + 2 * padH - (dH * (kH - 1) + 1)) / sH + 1;\n",
      "  int64_t output_width  = (input_width + 2 * padW - (dW * (kW - 1) + 1)) / sW + 1;\n",
      "  int64_t n_output_plane = n_input_plane * kW * kH;\n",
      "  int64_t outputLength = output_height * output_width;\n",
      "\n",
      "  output.resize_({ batch_size, n_output_plane, outputLength });\n",
      "  output.zero_();\n",
      "\n",
      "  Tensor *input_n = Tensor();\n",
      "  Tensor *output_n = Tensor();\n",
      "\n",
      "  for (int64_t elt = 0; elt < batch_size; elt++) {\n",
      "    THCTensor_(select)(input_n, input, 0, elt);\n",
      "    THCTensor_(select)(output_n, output, 0, elt);\n",
      "\n",
      "    im2col(\n",
      "      at::cuda::getCurrentCUDAStream(),\n",
      "      input_n.data(),\n",
      "      n_input_plane, input_height, input_width,\n",
      "      output_height, output_width,\n",
      "      kH, kW, padH, padW, sH, sW,\n",
      "      dH, dW, output_n).data();\n",
      "  }\n",
      "\n",
      "  THCTensor_(free)(input_n);\n",
      "  THCTensor_(free)(output_n);\n",
      "\n",
      "  if (!batched_input) {\n",
      "    output.resize_({ n_output_plane, outputLength });\n",
      "  }\n",
      "  THCTensor_(free)(input);\n",
      "}\n",
      "\n",
      "void Im2Col_updateGradInput(\n",
      "           \n",
      "           Tensor *grad_output,\n",
      "           Tensor *grad_input,\n",
      "           int64_t input_height, int64_t input_width,\n",
      "           int64_t kH, int64_t kW,\n",
      "           int64_t dH, int64_t dW,\n",
      "           int64_t padH, int64_t padW,\n",
      "           int64_t sH, int64_t sW) {\n",
      "\n",
      "  Col2Im_updateOutput(grad_output, grad_input,\n",
      "                             input_height, input_width,\n",
      "                             kH, kW, dH, dW,\n",
      "                             padH, padW, sH, sW);\n",
      "}\n",
      "\n",
      "#endif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# refresh output files\n",
    "create_aten_cuda_files(\n",
    "    output_path, \n",
    "    thcunn_path,\n",
    "    at_cuda_path,\n",
    "    zip(thcunn_files, at_cuda_files)\n",
    ")\n",
    "\n",
    "at_cuda_files_path = [\n",
    "    os.path.join(output_path, fn) \n",
    "    for fn in at_cuda_files\n",
    "]\n",
    "\n",
    "extra_rules = [\n",
    "    ('#include <THCUNN/upsampling.h>', '#include <ATen/cuda/UpSample.h>'),\n",
    "    ('#include <THCUNN/im2col.h>', '#include <ATen/cuda/Im2Col.h>')\n",
    "]\n",
    "cuda_th2at(at_cuda_files_path, extra_rules)\n",
    "!cat {output_path}/{at_cuda_files[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UpSampleBicubic2d.cu\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THNN_\\((.*)\\)\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCTensor_\\(size\\)\\(\\s*([^,]*),\\s*(.*)\\s*\\)\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCTensor_\\(nDimensionLegacyNoScalars\\)\\(\\s*(.*)\\s*\\)\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  [!](.*)->is_empty\\(\\)\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  (\\w)\\s*!=\\s*NULL\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCUNN_assertSameGPU\\([0-9]*,\\s*(.*)\\s*\\);\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCTensor_\\(zero\\)\\(\\s*(.*)\\s*\\)\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCTensor_\\(newContiguous\\)\\(\\s*(.*)\\s*\\);\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  accscalar_t\\(\\s*(.*)\\s*\\)\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCNumerics\\<scalar_t\\>::ne\\(\\s*(.*),\\s*(.*)\\s*\\)\\s*\n",
      "--------------------------------------------------------------------------------\n",
      "rule:  THCCeilDiv\\(\\s*([^()]*),\\s*([^()]*)\\s*\\)\n",
      "replace:  THCCeilDiv(outputPlaneSize, 128)  by:  (outputPlaneSize + 128-1) / 128\n"
     ]
    }
   ],
   "source": [
    "for fn in at_cuda_files:\n",
    "    print('=' * 80)\n",
    "    print(fn)\n",
    "    \n",
    "    # with open(os.path.join(output_path, fn), 'r') as f:\n",
    "    #     text = f.read()\n",
    "    text = '''\n",
    "      dim3 grid(THCCeilDiv(outputPlaneSize, 128),\n",
    "            devInput.getSize(1),\n",
    "            devInput.getSize(0));\n",
    "    '''\n",
    "    _rules = [\n",
    "        (r'THNN_\\((.*)\\)', None),\n",
    "        (r'THCTensor_\\(size\\)\\(\\s*([^,]*),\\s*(.*)\\s*\\)', '{}.size({})'),\n",
    "        (r'THCTensor_\\(nDimensionLegacyNoScalars\\)\\(\\s*(.*)\\s*\\)', '{}.ndimension()'),\n",
    "        (r'[!](.*)->is_empty\\(\\)', '{}.numel() != 0'),\n",
    "        (r'(\\w)\\s*!=\\s*NULL', '{}.defined()'),\n",
    "        (r'THCUNN_assertSameGPU\\([0-9]*,\\s*(.*)\\s*\\);', \n",
    "         'checkAllSameGPU(\"check_all_same_gpu\", {{ {} }}); // TODO: use the name of the function as description here'),\n",
    "        (r'THCTensor_\\(zero\\)\\(\\s*(.*)\\s*\\)', '{0}.zero_()'),\n",
    "        (r'THCTensor_\\(newContiguous\\)\\(\\s*(.*)\\s*\\);', \n",
    "         'auto {0} = {0}_.contiguous(); // TODO: add _ to the variable definition above'),\n",
    "        (r'accscalar_t\\(\\s*(.*)\\s*\\)', 'static_cast<accscalar_t>({})'),\n",
    "        (r'THCNumerics\\<scalar_t\\>::ne\\(\\s*(.*),\\s*(.*)\\s*\\)\\s*', '{} != {}'),\n",
    "        (r'THCCeilDiv\\(\\s*([^()]*),\\s*([^()]*)\\s*\\)', '({0} + {1}-1) / {1}')\n",
    "    ]\n",
    "\n",
    "    for rule, output_format in _rules:\n",
    "        print('-' * 80)\n",
    "        print('rule: ', rule )\n",
    "        result = re.finditer(rule, text, re.MULTILINE)\n",
    "\n",
    "        for r in result:\n",
    "            _in = r.group(0)\n",
    "\n",
    "            if output_format is None:\n",
    "                _out = r.group(1)\n",
    "            else:\n",
    "                try:\n",
    "                    _out = output_format.format(*r.groups())\n",
    "                except:\n",
    "                    print('[EE]', r.groups())\n",
    "            text = text.replace(_in, apply_rules(RULES_NAME_EXTRA, _out))\n",
    "            \n",
    "            print('replace: ', _in, ' by: ', apply_rules(RULES_NAME_EXTRA, _out))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '/home/xmn/dev/quansight/pytorch-project/pytorch/aten/src/ATen/native/cuda/UpSample.h'\n",
    "\n",
    "at_cuda_files_path = [fn]\n",
    "\n",
    "cuda_th2at(at_cuda_files_path)\n",
    "!cat {fn}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
